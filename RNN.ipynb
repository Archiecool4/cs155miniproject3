{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4h8AGWXMQRs",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF1y8MF-MC5H",
        "colab_type": "code",
        "outputId": "aab86a0b-7047-4091-ac99-82198a8c5e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVurxFszMeK5",
        "colab_type": "text"
      },
      "source": [
        "# Get Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Lc-c_sMSS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load text\n",
        "file = open('shakespeare.txt', 'r')\n",
        "sonnets = file.read()\n",
        "file.close()\n",
        "\n",
        "# Strip whitespace and numbers (leaves only words)\n",
        "tokens = sonnets.split()\n",
        "all_words = ' '.join(i for i in tokens if not i.isdigit())\n",
        "all_words = all_words.lower()\n",
        "\n",
        "# Break into sequences of 40 characters each, starting for each group of 10\n",
        "N = 5\n",
        "seq_length = 41\n",
        "sequences = list()\n",
        "for i in range(0, len(all_words), N):\n",
        "  seq = all_words[i : min(i + seq_length, len(all_words))] # Prevent out-of-bounds error\n",
        "  if len(seq) == seq_length:\n",
        "    sequences.append(seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQv7hOaZZbMq",
        "colab_type": "text"
      },
      "source": [
        "# Get int Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VQfoayfMd6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get unique characters and match them to an index\n",
        "chars = sorted(list(set(all_words)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY9zRiA7Zl2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sequences of ints\n",
        "int_sequences = list()\n",
        "for seq in sequences:\n",
        "\tint_seq = [mapping[char] for char in seq]\n",
        "\tint_sequences.append(int_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddt32VaOfdR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separate into X and Y sets (40 characters are input, 1 is output)\n",
        "int_sequences = np.array(int_sequences)\n",
        "X, y = int_sequences[:,:-1], int_sequences[:,-1]\n",
        "\n",
        "# One-hot encode them\n",
        "class_num = len(mapping)\n",
        "int_sequences = [to_categorical(x, num_classes = class_num) for x in X]\n",
        "X = np.array(int_sequences)\n",
        "y = to_categorical(y, num_classes = class_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g17w4xl9fYXC",
        "colab_type": "text"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oBkk-fueb1u",
        "colab_type": "code",
        "outputId": "6cb20844-c750-4af8-8348-c4b024552193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Model with LSTM layer of 175 units and softmax activation on output layer\n",
        "model = tf.keras.Sequential()\n",
        "model.add(layers.LSTM(175, input_shape=(X.shape[1], X.shape[2]), dropout=0.1))\n",
        "model.add(layers.Dense(50, activation= 'relu'))\n",
        "model.add(layers.Dense(class_num, activation='softmax'))\n",
        "\n",
        "# Define loss function and train\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, batch_size = 32, epochs = 100)\n",
        "print(model.summary())\n",
        "\n",
        "# Save weights \n",
        "saved_weights = [layer.get_weights() for layer in model.layers]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 2.8313 - accuracy: 0.2190\n",
            "Epoch 2/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 2.4192 - accuracy: 0.3082\n",
            "Epoch 3/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 2.2720 - accuracy: 0.3358\n",
            "Epoch 4/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 2.1837 - accuracy: 0.3596\n",
            "Epoch 5/100\n",
            "586/586 [==============================] - 44s 76ms/step - loss: 2.1121 - accuracy: 0.3744\n",
            "Epoch 6/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 2.0635 - accuracy: 0.3850\n",
            "Epoch 7/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 2.0045 - accuracy: 0.4019\n",
            "Epoch 8/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.9612 - accuracy: 0.4133\n",
            "Epoch 9/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.9171 - accuracy: 0.4226\n",
            "Epoch 10/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.8848 - accuracy: 0.4301\n",
            "Epoch 11/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 1.8545 - accuracy: 0.4416\n",
            "Epoch 12/100\n",
            "586/586 [==============================] - 45s 77ms/step - loss: 1.8136 - accuracy: 0.4492\n",
            "Epoch 13/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 1.7738 - accuracy: 0.4609\n",
            "Epoch 14/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 1.7429 - accuracy: 0.4695\n",
            "Epoch 15/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 1.7050 - accuracy: 0.4780\n",
            "Epoch 16/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 1.6709 - accuracy: 0.4899\n",
            "Epoch 17/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.6198 - accuracy: 0.5009\n",
            "Epoch 18/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.5840 - accuracy: 0.5150\n",
            "Epoch 19/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 1.5472 - accuracy: 0.5243\n",
            "Epoch 20/100\n",
            "586/586 [==============================] - 43s 73ms/step - loss: 1.5077 - accuracy: 0.5336\n",
            "Epoch 21/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.4727 - accuracy: 0.5430\n",
            "Epoch 22/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.4304 - accuracy: 0.5603\n",
            "Epoch 23/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.3902 - accuracy: 0.5683\n",
            "Epoch 24/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.3537 - accuracy: 0.5796\n",
            "Epoch 25/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.3108 - accuracy: 0.5898\n",
            "Epoch 26/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 1.2891 - accuracy: 0.6011\n",
            "Epoch 27/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.2485 - accuracy: 0.6147\n",
            "Epoch 28/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.2157 - accuracy: 0.6241\n",
            "Epoch 29/100\n",
            "586/586 [==============================] - 43s 73ms/step - loss: 1.1887 - accuracy: 0.6274\n",
            "Epoch 30/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.1467 - accuracy: 0.6409\n",
            "Epoch 31/100\n",
            "586/586 [==============================] - 43s 73ms/step - loss: 1.1276 - accuracy: 0.6483\n",
            "Epoch 32/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.0941 - accuracy: 0.6613\n",
            "Epoch 33/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 1.0778 - accuracy: 0.6682\n",
            "Epoch 34/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 1.0451 - accuracy: 0.6775\n",
            "Epoch 35/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.0345 - accuracy: 0.6843\n",
            "Epoch 36/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 1.0107 - accuracy: 0.6887\n",
            "Epoch 37/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 0.9821 - accuracy: 0.6963\n",
            "Epoch 38/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 0.9709 - accuracy: 0.7019\n",
            "Epoch 39/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.9562 - accuracy: 0.7051\n",
            "Epoch 40/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.9287 - accuracy: 0.7150\n",
            "Epoch 41/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.9082 - accuracy: 0.7207\n",
            "Epoch 42/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.8975 - accuracy: 0.7246\n",
            "Epoch 43/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.8914 - accuracy: 0.7292\n",
            "Epoch 44/100\n",
            "586/586 [==============================] - 44s 76ms/step - loss: 0.8764 - accuracy: 0.7331\n",
            "Epoch 45/100\n",
            "586/586 [==============================] - 44s 76ms/step - loss: 0.8419 - accuracy: 0.7399\n",
            "Epoch 46/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.8331 - accuracy: 0.7462\n",
            "Epoch 47/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.8143 - accuracy: 0.7512\n",
            "Epoch 48/100\n",
            "586/586 [==============================] - 44s 76ms/step - loss: 0.8063 - accuracy: 0.7522\n",
            "Epoch 49/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 0.7913 - accuracy: 0.7595\n",
            "Epoch 50/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 0.7893 - accuracy: 0.7579\n",
            "Epoch 51/100\n",
            "586/586 [==============================] - 43s 73ms/step - loss: 0.7663 - accuracy: 0.7633\n",
            "Epoch 52/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 0.7574 - accuracy: 0.7692\n",
            "Epoch 53/100\n",
            "586/586 [==============================] - 43s 73ms/step - loss: 0.7588 - accuracy: 0.7711\n",
            "Epoch 54/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 0.7217 - accuracy: 0.7821\n",
            "Epoch 55/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 0.7252 - accuracy: 0.7802\n",
            "Epoch 56/100\n",
            "586/586 [==============================] - 43s 73ms/step - loss: 0.7146 - accuracy: 0.7854\n",
            "Epoch 57/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.7041 - accuracy: 0.7874\n",
            "Epoch 58/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.6980 - accuracy: 0.7921\n",
            "Epoch 59/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.6857 - accuracy: 0.7919\n",
            "Epoch 60/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.6838 - accuracy: 0.7955\n",
            "Epoch 61/100\n",
            "586/586 [==============================] - 44s 76ms/step - loss: 0.6666 - accuracy: 0.7999\n",
            "Epoch 62/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.6614 - accuracy: 0.8022\n",
            "Epoch 63/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.6802 - accuracy: 0.7940\n",
            "Epoch 64/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 0.6575 - accuracy: 0.8025\n",
            "Epoch 65/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 0.6500 - accuracy: 0.8056\n",
            "Epoch 66/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 0.6143 - accuracy: 0.8165\n",
            "Epoch 67/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 0.6277 - accuracy: 0.8101\n",
            "Epoch 68/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.6255 - accuracy: 0.8147\n",
            "Epoch 69/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.6148 - accuracy: 0.8140\n",
            "Epoch 70/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.6062 - accuracy: 0.8197\n",
            "Epoch 71/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.6062 - accuracy: 0.8140\n",
            "Epoch 72/100\n",
            "586/586 [==============================] - 45s 76ms/step - loss: 0.6141 - accuracy: 0.8176\n",
            "Epoch 73/100\n",
            "586/586 [==============================] - 44s 76ms/step - loss: 0.5850 - accuracy: 0.8260\n",
            "Epoch 74/100\n",
            "586/586 [==============================] - 44s 76ms/step - loss: 0.5932 - accuracy: 0.8211\n",
            "Epoch 75/100\n",
            "586/586 [==============================] - 45s 76ms/step - loss: 0.5795 - accuracy: 0.8269\n",
            "Epoch 76/100\n",
            "586/586 [==============================] - 45s 77ms/step - loss: 0.5769 - accuracy: 0.8263\n",
            "Epoch 77/100\n",
            "586/586 [==============================] - 45s 77ms/step - loss: 0.5835 - accuracy: 0.8246\n",
            "Epoch 78/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5790 - accuracy: 0.8284\n",
            "Epoch 79/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5723 - accuracy: 0.8277\n",
            "Epoch 80/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 0.5555 - accuracy: 0.8326\n",
            "Epoch 81/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5698 - accuracy: 0.8258\n",
            "Epoch 82/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5560 - accuracy: 0.8324\n",
            "Epoch 83/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5571 - accuracy: 0.8322\n",
            "Epoch 84/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5435 - accuracy: 0.8381\n",
            "Epoch 85/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5468 - accuracy: 0.8360\n",
            "Epoch 86/100\n",
            "586/586 [==============================] - 44s 76ms/step - loss: 0.5319 - accuracy: 0.8423\n",
            "Epoch 87/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5162 - accuracy: 0.8446\n",
            "Epoch 88/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 0.5331 - accuracy: 0.8400\n",
            "Epoch 89/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5291 - accuracy: 0.8427\n",
            "Epoch 90/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5165 - accuracy: 0.8481\n",
            "Epoch 91/100\n",
            "586/586 [==============================] - 43s 74ms/step - loss: 0.5261 - accuracy: 0.8419\n",
            "Epoch 92/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5239 - accuracy: 0.8453\n",
            "Epoch 93/100\n",
            "586/586 [==============================] - 44s 74ms/step - loss: 0.5138 - accuracy: 0.8437\n",
            "Epoch 94/100\n",
            "586/586 [==============================] - 44s 76ms/step - loss: 0.5031 - accuracy: 0.8499\n",
            "Epoch 95/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5049 - accuracy: 0.8478\n",
            "Epoch 96/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.4876 - accuracy: 0.8524\n",
            "Epoch 97/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.5029 - accuracy: 0.8495\n",
            "Epoch 98/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.4797 - accuracy: 0.8563\n",
            "Epoch 99/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.4952 - accuracy: 0.8507\n",
            "Epoch 100/100\n",
            "586/586 [==============================] - 44s 75ms/step - loss: 0.4864 - accuracy: 0.8541\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_8 (LSTM)                (None, 175)               149100    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 50)                8800      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 37)                1887      \n",
            "=================================================================\n",
            "Total params: 159,787\n",
            "Trainable params: 159,787\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgbYBCGelobH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save Model\n",
        "model_json = model.to_json()\n",
        "with open('model3.json', 'w') as json_file:\n",
        "  json_file.write(model_json)\n",
        "model.save_weights('model3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmjsknjIlsp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Model\n",
        "json_file = open('model3.json', 'r')\n",
        "model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = tf.keras.models.model_from_json(model_json)\n",
        "model.load_weights('model3.h5')\n",
        "\n",
        "# Recompile Model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "saved_weights = [layer.get_weights() for layer in model.layers]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UwwGzleo4h1",
        "colab_type": "text"
      },
      "source": [
        "# Generate Poem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU5-g4-3Shhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement temperature parameter\n",
        "temp = 0.75\n",
        "\n",
        "# Model with Lambda layer for temperature\n",
        "temp_model = tf.keras.Sequential()\n",
        "temp_model.add(layers.LSTM(175, input_shape=(X.shape[1], X.shape[2]), dropout=0.1))\n",
        "temp_model.add(layers.Dense(50, activation= 'relu'))\n",
        "temp_model.add(layers.Lambda(lambda x: x / temp))\n",
        "temp_model.add(layers.Dense(class_num, activation='softmax'))\n",
        "\n",
        "# Load saved weights (skipping lambda layer)\n",
        "temp_model.layers[0].set_weights(saved_weights[0])\n",
        "temp_model.layers[1].set_weights(saved_weights[1])\n",
        "temp_model.layers[3].set_weights(saved_weights[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enbzhud4iNqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "\n",
        "  in_text = seed_text\n",
        "\n",
        "  for i in range(n_chars):\n",
        "    encoded = [mapping[char] for char in in_text]\n",
        "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "    encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "\n",
        "    yhat = model.predict_classes(encoded)\n",
        "\n",
        "    out_char = ''\n",
        "    for char, index in mapping.items():\n",
        "      if index == yhat:\n",
        "        out_char = char\n",
        "        break\n",
        "\n",
        "    in_text += out_char\n",
        "\n",
        "  return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q72dYCSZjKdf",
        "colab_type": "code",
        "outputId": "5a7455ab-f040-4e36-a22d-c2c9b5f24031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(generate_seq(temp_model, mapping, 40, 'shall i compare thee to a summer\\'s day?', 14*40))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summer's day? why deft ere but the end, that me nar freme of love, but what wards the should fartered there i am hus were broe, where flager happy fams, and you in whose will in thy self thou save, who are to not free inshorns of their remoring: woo of what it dest some conched as have in liveryino of not reauty being no corrt, me for my love thee, as prove of moly rese, om day, yee in the remory to on these badty though mese feerd, then my own forgh, and 'tiff oo heavi, and therefire wishom me mose foll time and of triies of remove, who i ame seef, touth tris r paig\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT5_Sx8KgoYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}